{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset files\n",
    "comment_files = ['Youtube02-KatyPerry.csv','Youtube01-Psy.csv','Youtube05-Shakira.csv','Youtube04-Eminem.csv','Youtube03-LMFAO.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z12pgdhovmrktzm3i23es5d5junftft3f</td>\n",
       "      <td>lekanaVEVO1</td>\n",
       "      <td>2014-07-22T15:27:50</td>\n",
       "      <td>i love this so much. AND also I Generate Free ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z13yx345uxepetggz04ci5rjcxeohzlrtf4</td>\n",
       "      <td>Pyunghee</td>\n",
       "      <td>2014-07-27T01:57:16</td>\n",
       "      <td>http://www.billboard.com/articles/columns/pop-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z12lsjvi3wa5x1vwh04cibeaqnzrevxajw00k</td>\n",
       "      <td>Erica Ross</td>\n",
       "      <td>2014-07-27T02:51:43</td>\n",
       "      <td>Hey guys! Please join me in my fight to help a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jcjuovxbwfr0ge04cev2ipsjdfdurwck</td>\n",
       "      <td>Aviel Haimov</td>\n",
       "      <td>2014-08-01T12:27:48</td>\n",
       "      <td>http://psnboss.com/?ref=2tGgp3pV6L this is the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13qybua2yfydzxzj04cgfpqdt2syfx53ms0k</td>\n",
       "      <td>John Bello</td>\n",
       "      <td>2014-08-01T21:04:03</td>\n",
       "      <td>Hey everyone. Watch this trailer!!!!!!!!  http...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              COMMENT_ID        AUTHOR                 DATE  \\\n",
       "0      z12pgdhovmrktzm3i23es5d5junftft3f   lekanaVEVO1  2014-07-22T15:27:50   \n",
       "1    z13yx345uxepetggz04ci5rjcxeohzlrtf4      Pyunghee  2014-07-27T01:57:16   \n",
       "2  z12lsjvi3wa5x1vwh04cibeaqnzrevxajw00k    Erica Ross  2014-07-27T02:51:43   \n",
       "3    z13jcjuovxbwfr0ge04cev2ipsjdfdurwck  Aviel Haimov  2014-08-01T12:27:48   \n",
       "4  z13qybua2yfydzxzj04cgfpqdt2syfx53ms0k    John Bello  2014-08-01T21:04:03   \n",
       "\n",
       "                                             CONTENT  CLASS  \n",
       "0  i love this so much. AND also I Generate Free ...      1  \n",
       "1  http://www.billboard.com/articles/columns/pop-...      1  \n",
       "2  Hey guys! Please join me in my fight to help a...      1  \n",
       "3  http://psnboss.com/?ref=2tGgp3pV6L this is the...      1  \n",
       "4  Hey everyone. Watch this trailer!!!!!!!!  http...      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding dataset file's into a pandas dataframe\n",
    "dfex = []\n",
    "for x in comment_files:\n",
    "    data = pd.read_csv(x)\n",
    "    dfex.append(data)\n",
    "dfex = pd.concat(dfex)\n",
    "dfex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      COMMENT_ID  \\\n",
      "0              z12rwfnyyrbsefonb232i5ehdxzkjzjs2   \n",
      "2              z13vsfqirtavjvu0t22ezrgzyorwxhpf3   \n",
      "4              z13xjfr42z3uxdz2223gx5rrzs3dt5hna   \n",
      "6              z12zgrw5furdsn0sc233hfwavnznyhicq   \n",
      "8          z12gxdortqzwhhqas04cfjrwituzghb5tvk0k   \n",
      "..                                           ...   \n",
      "440  LneaDw26bFv4NFg1m91sL1Aq1p-2N06VweQT4vOwTqc   \n",
      "442  LneaDw26bFsVQRSG0ecctIfRIn4Od1tJqvnhDwCNqbI   \n",
      "443  LneaDw26bFu3RCmyrWyP9S6wh1h9dBv3X95g1HzKAb4   \n",
      "444  LneaDw26bFsD65dtIvAEObWYIYnFTqQDKBek_Ypz3J8   \n",
      "445  LneaDw26bFuvs-8oWkLpAFa6g3QHpWD8k7sbbMP3Bg8   \n",
      "\n",
      "                                 AUTHOR DATE  \\\n",
      "0                           Lisa Wellas  NaN   \n",
      "2                            Ajkal Khan  NaN   \n",
      "4                           Jihad Naser  NaN   \n",
      "6                              kyeman13  NaN   \n",
      "8                  Muhammad Asim Mansha  NaN   \n",
      "..                                  ...  ...   \n",
      "440  Ando Nesia - | MC | Music Producer  NaN   \n",
      "442                            the34104  NaN   \n",
      "443                             Dany PK  NaN   \n",
      "444                   SmexyFriedChicken  NaN   \n",
      "445      The Guy That's Done Everything  NaN   \n",
      "\n",
      "                                               CONTENT  CLASS  \n",
      "0            +447935454150 lovely girl talk to me xxx﻿      1  \n",
      "2    my sister just received over 6,500 new <a rel=...      1  \n",
      "4                       Hello I&#39;am from Palastine﻿      1  \n",
      "6    Go check out my rapping video called Four Whee...      1  \n",
      "8                       Aslamu Lykum... From Pakistan﻿      1  \n",
      "..                                                 ...    ...  \n",
      "440  DO YOU KNOW HOW SEAN KINGSTON GOT FAMOUS WHY D...      1  \n",
      "442  check out eminem latest track survival if u didnt      1  \n",
      "443           SUBSCRIBE TO MY CHANNEL X PLEASE!. SPARE      1  \n",
      "444  Check out my videos guy! :) Hope you guys had ...      1  \n",
      "445  3 yrs ago I had a health scare but thankfully ...      1  \n",
      "\n",
      "[245 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking for NULL values \n",
    "HAS_NULL = dfex.isnull()\n",
    "ROWS_WITH_NAN = dfex[HAS_NULL.any(axis=1)]\n",
    "print(ROWS_WITH_NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love this so much. AND also I Generate Free ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.billboard.com/articles/columns/pop-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey guys! Please join me in my fight to help a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://psnboss.com/?ref=2tGgp3pV6L this is the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey everyone. Watch this trailer!!!!!!!!  http...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             CONTENT  CLASS\n",
       "0  i love this so much. AND also I Generate Free ...      1\n",
       "1  http://www.billboard.com/articles/columns/pop-...      1\n",
       "2  Hey guys! Please join me in my fight to help a...      1\n",
       "3  http://psnboss.com/?ref=2tGgp3pV6L this is the...      1\n",
       "4  Hey everyone. Watch this trailer!!!!!!!!  http...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unrelated columns\n",
    "df = dfex.drop(['COMMENT_ID','AUTHOR','DATE'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [CONTENT, CLASS]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Again checking for NULL values\n",
    "HAS_NULL2 = df.isnull()\n",
    "ROWS_WITH_NAN2 = df[HAS_NULL2.any(axis=1)]\n",
    "print(ROWS_WITH_NAN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "1        1005\n",
       "0         951\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(['CLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['CONTENT'], df['CLASS'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240                                   Shakira I love you\n",
      "417     are  there  people who like  this shit? hahah...\n",
      "81                                          Very Nice !﻿\n",
      "205    I know that maybe no one will read this but PL...\n",
      "53                   WE GO FOR 1,000,000,000 FOR EMINEM﻿\n",
      "Name: CONTENT, dtype: object 240    0\n",
      "417    0\n",
      "81     0\n",
      "205    1\n",
      "53     0\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head(), y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB # alpha = 0.1\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably      0.134032\n",
      "spam          0.165381\n",
      "making        0.192129\n",
      "hello         0.193778\n",
      "today         0.197145\n",
      "month         0.206035\n",
      "working       0.212071\n",
      "any           0.213335\n",
      "these         0.221679\n",
      "nothing       0.228040\n",
      "take          0.235892\n",
      "anyone        0.240294\n",
      "chance        0.247872\n",
      "dream         0.247947\n",
      "thank         0.254816\n",
      "trying        0.258867\n",
      "appreciate    0.260044\n",
      "first         0.273844\n",
      "getting       0.280303\n",
      "website       0.286403\n",
      "dtype: float64 wow          1.0\n",
      "the          1.0\n",
      "super        1.0\n",
      "style        1.0\n",
      "still        1.0\n",
      "shakira      1.0\n",
      "party        1.0\n",
      "nice         1.0\n",
      "lt           1.0\n",
      "love         1.0\n",
      "like         1.0\n",
      "http         1.0\n",
      "good         1.0\n",
      "from         1.0\n",
      "cool         1.0\n",
      "but          1.0\n",
      "br           1.0\n",
      "billion      1.0\n",
      "beautiful    1.0\n",
      "awesome      1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Printing smallest and largest tfidfvalues of tokens\n",
    "Wvect = TfidfVectorizer(min_df=5).fit(X_test)\n",
    "t_X_train = Wvect.transform(X_test)   \n",
    "sorted_tfidf_index = t_X_train.max(axis = 0).toarray()[0].argsort()\n",
    "sorted_tfidf_value = np.sort(t_X_train.max(axis = 0).toarray()[0])\n",
    "\n",
    "feature_names = np.array(Wvect.get_feature_names())\n",
    "    \n",
    "smallest_tfidfs = feature_names[sorted_tfidf_index[:20]]\n",
    "smallest_tfidfs_vals = sorted_tfidf_value[:20] \n",
    "smallest_tfidfs_series = pd.Series(data = smallest_tfidfs_vals, index = smallest_tfidfs)\n",
    "smallest_tfidfs_series = smallest_tfidfs_series.iloc[np.lexsort([smallest_tfidfs_series.index, smallest_tfidfs_series.values])]\n",
    "    \n",
    "    \n",
    "largest_tfidfs = feature_names[sorted_tfidf_index[-20:]]\n",
    "largest_tfidfs_vals = sorted_tfidf_value[-20:]\n",
    "largest_tfidfs_series = pd.Series(data = largest_tfidfs_vals, index = largest_tfidfs)\n",
    "largest_tfidfs_series = largest_tfidfs_series.iloc[np.lexsort([largest_tfidfs_series.index, largest_tfidfs_series.values])[::-1]]\n",
    "    \n",
    "print(smallest_tfidfs_series, largest_tfidfs_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9219628514056225\n"
     ]
    }
   ],
   "source": [
    "# Using Multinomial Naive bayes for classification\n",
    "\n",
    "vect2 = TfidfVectorizer(min_df=5,ngram_range = (1,3)).fit(X_train)\n",
    "t_X_train = vect2.transform(X_train)\n",
    "t_X_test = vect2.transform(X_test)\n",
    "\n",
    "model = MultinomialNB(alpha = 0.1)\n",
    "\n",
    "model.fit(t_X_train, y_train)\n",
    "\n",
    "pred = model.predict(t_X_test)\n",
    "score = roc_auc_score(y_test,pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "test1= [\"Check out my page. Free games.\", \"This is so cool.\", \"Check my song for better version.\",\"www.iiits.ac.in\"]\n",
    "vec = vect2.transform(test1)\n",
    "output1 = model.predict(vec)\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9571536144578313\n"
     ]
    }
   ],
   "source": [
    "# Using Logistic Regression for classification\n",
    "\n",
    "vect3 = TfidfVectorizer(min_df=5, ngram_range = (1,3)).fit(X_train)\n",
    "    \n",
    "tX_train = vect3.transform(X_train)\n",
    "tX_test = vect3.transform(X_test)\n",
    "\n",
    "model2 = LogisticRegression(C = 100)\n",
    "\n",
    "model2.fit(tX_train, y_train)\n",
    "pred2 = model2.predict(tX_test)    \n",
    "score = roc_auc_score(y_test, pred2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "test1= [\"Check out my page. Free games.\", \"This is so cool.\", \"Check my song for better version.\",\"www.iiits.ac.in\"]\n",
    "vec = vect3.transform(test1)\n",
    "output1 = model2.predict(vec)\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9594628514056225\n"
     ]
    }
   ],
   "source": [
    "# Using SVM for classification\n",
    "\n",
    "vect4 = TfidfVectorizer(min_df=5, ngram_range = (1,3)).fit(X_train)\n",
    "t_X_train2 = vect4.transform(X_train)\n",
    "t_X_test2 = vect4.transform(X_test)\n",
    "\n",
    "model3 = SVC(C=10000)\n",
    "model3.fit(t_X_train2, y_train)\n",
    "\n",
    "pred3 = model3.predict(t_X_test2)\n",
    "score = roc_auc_score(y_test, pred3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "test1= [\"Check out my page. Free games.\", \"This is so cool.\", \"Check my song for better version.\",\"www.iiits.ac.in\"]\n",
    "vec = vect4.transform(test1)\n",
    "output1 = model3.predict(vec)\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "\n",
    "1. Adding character ngrams.\n",
    "2. Adding more features like length of comment, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_spam_model.sav'\n",
    "pickle.dump(model2, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_file = 'vectorizer.sav'\n",
    "pickle.dump(vect2, open(vect_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
